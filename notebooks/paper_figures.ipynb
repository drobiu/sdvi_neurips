{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\drobi\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
      "     -------------------------------------- 294.6/294.6 kB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from seaborn) (1.25.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from seaborn) (3.7.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.25.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\drobi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.12.2\n",
      "    Uninstalling seaborn-0.12.2:\n",
      "      Successfully uninstalled seaborn-0.12.2\n",
      "Successfully installed seaborn-0.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drobi\\Desktop\\uni\\sdvi_neurips\\notebooks\\..\\run_baselines.py:302: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"baselines_config\", config_name=\"config\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import hydra\n",
    "import omegaconf\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.gp as gp\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from models.pyro_extensions.infer import SDVI\n",
    "from models.pyro_extensions.resource_allocation import SuccessiveHalving\n",
    "from models import normal_model\n",
    "from run_baselines import NormalModel\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "matplotlib.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycler\n",
    "line_cycler = (cycler(color=[\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#F0E442\"]) +\n",
    "               cycler(linestyle=[\"-\", \"--\", \"-.\", \":\", \"-\", \"--\", \"-.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo2config = {\n",
    "    \"DCC\": {\"color\": \"#E69F00\", \"linestyle\": \"--\"},\n",
    "    \"BBVI\": {\"color\": \"#56B4E9\", \"linestyle\": \"-.\"},\n",
    "    \"Pyro AutoGuide\": {\"color\": \"#009E73\", \"linestyle\": \":\"},\n",
    "    \"SDVI\": {\"color\": \"#D55E00\", \"linestyle\": \"-\"},\n",
    "    \"Stochastic SDVI\": {\"color\": \"#0072B2\", \"linestyle\": \":\"},\n",
    "    \"S-SDVI\": {\"color\": \"#0072B2\", \"linestyle\": \":\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "major = 8.0\n",
    "minor = 5.0\n",
    "\n",
    "major_tick_width = 2.0\n",
    "\n",
    "update_rc_params = {\n",
    "    'font.family': \"serif\", \n",
    "    'font.size': 24, \n",
    "    'legend.fontsize': 15,\n",
    "    'text.usetex': True,\n",
    "    'xtick.major.size': major,\n",
    "    'xtick.major.width': major_tick_width,\n",
    "    'xtick.minor.size': minor,\n",
    "    'ytick.major.size': major,\n",
    "    'ytick.major.width': major_tick_width,\n",
    "    'ytick.minor.size': minor,\n",
    "    'axes.linewidth': 2.0,\n",
    "}\n",
    "\n",
    "plt.rcParams.update(update_rc_params)\n",
    "plt.rc(\"axes\", prop_cycle=line_cycler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH1_PRIOR_MEAN = -3\n",
    "BRANCH2_PRIOR_MEAN = 3\n",
    "PRIOR_STD = 1\n",
    "LIKELIHOOD_STD = 2\n",
    "\n",
    "OBSERVED_DATA = 2\n",
    "\n",
    "def marginal_likelihood(data, likelihood_std, prior_mean, prior_std):\n",
    "    \"\"\"Calculate the marginal likelihood of a branch. Assumes we observe only\n",
    "    a single data point.\n",
    "\n",
    "    Taken from Section 2.5 at https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf.\n",
    "    \"\"\"\n",
    "    likelihood_var = math.pow(likelihood_std, 2)\n",
    "    prior_var = math.pow(prior_std, 2)\n",
    "\n",
    "    first_term = likelihood_std / (\n",
    "        (math.sqrt(2 * math.pi) * likelihood_std)\n",
    "        * math.sqrt(prior_var + likelihood_var)\n",
    "    )\n",
    "    second_term = math.exp(\n",
    "        -(math.pow(data, 2) / (2 * likelihood_var))\n",
    "        - (math.pow(prior_mean, 2) / (2 * prior_var))\n",
    "    )\n",
    "    third_term = math.exp(\n",
    "        (\n",
    "            (prior_var * math.pow(data, 2) / likelihood_var)\n",
    "            + (likelihood_var * math.pow(prior_mean, 2) / prior_var)\n",
    "            + 2 * data * prior_mean\n",
    "        )\n",
    "        / (2 * (prior_var + likelihood_var))\n",
    "    )\n",
    "    return first_term * second_term * third_term\n",
    "\n",
    "\n",
    "def posterior_params(data, likelihood_std, prior_mean, prior_std):\n",
    "    \"\"\"Calculate the posterior mean and standard deviation of a branch. Assumes we\n",
    "    observe only a single data point.\"\"\"\n",
    "    prior_precision = 1 / math.pow(prior_std, 2)\n",
    "    likelihood_precision = 1 / math.pow(likelihood_std, 2)\n",
    "    post_mean = (prior_precision * prior_mean + likelihood_precision * data) / (\n",
    "        prior_precision + likelihood_precision\n",
    "    )\n",
    "    post_std = 1 / (prior_precision + likelihood_precision)\n",
    "    return post_mean, post_std\n",
    "\n",
    "class ToyModel:\n",
    "    observed_data = torch.tensor(OBSERVED_DATA)\n",
    "\n",
    "    branch1_prior_mean = BRANCH1_PRIOR_MEAN\n",
    "    branch2_prior_mean = BRANCH2_PRIOR_MEAN\n",
    "    prior_std = PRIOR_STD\n",
    "    likelihood_std = LIKELIHOOD_STD\n",
    "\n",
    "    def __init__(self, cut_point=0.0):\n",
    "        self.branch1_post_mean, self.branch1_post_std = posterior_params(\n",
    "            self.observed_data,\n",
    "            self.likelihood_std,\n",
    "            self.branch1_prior_mean,\n",
    "            self.prior_std,\n",
    "        )\n",
    "        self.branch1_Z = marginal_likelihood(\n",
    "            self.observed_data,\n",
    "            self.likelihood_std,\n",
    "            self.branch1_prior_mean,\n",
    "            self.prior_std,\n",
    "        )\n",
    "        self.branch2_post_mean, self.branch2_post_std = posterior_params(\n",
    "            self.observed_data,\n",
    "            self.likelihood_std,\n",
    "            self.branch2_prior_mean,\n",
    "            self.prior_std,\n",
    "        )\n",
    "        self.branch2_Z = marginal_likelihood(\n",
    "            self.observed_data,\n",
    "            self.likelihood_std,\n",
    "            self.branch2_prior_mean,\n",
    "            self.prior_std,\n",
    "        )\n",
    "\n",
    "        self.cut_point = cut_point\n",
    "\n",
    "        z0_prior = dist.Normal(0, 1)\n",
    "        self.branch1_prior = z0_prior.cdf(torch.tensor(cut_point))\n",
    "        self.marginal_likelihood = self.branch1_prior * self.branch1_Z + (1 - self.branch1_prior) * self.branch2_Z\n",
    "\n",
    "        self.branch1_post_prob = (self.branch1_prior * self.branch1_Z) / self.marginal_likelihood\n",
    "\n",
    "    def __call__(self):\n",
    "        z0 = pyro.sample(\"z0\", dist.Normal(0, 1))\n",
    "        if z0 < self.cut_point:\n",
    "            z1 = pyro.sample(\"z1\", dist.Normal(self.branch1_prior_mean, self.prior_std))\n",
    "        else:\n",
    "            z1 = pyro.sample(\"z2\", dist.Normal(self.branch2_prior_mean, self.prior_std))\n",
    "\n",
    "        x = pyro.sample(\n",
    "            \"x\", dist.Normal(z1, self.likelihood_std), obs=self.observed_data\n",
    "        )\n",
    "        return z0.item(), z1, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_weights(file_list):\n",
    "    weight_keys = [\"br2_weights_pyro\", \"br2_weights_bbvi\", \"br2_weights_sdvi\"]\n",
    "    weights_dict = {\n",
    "        \"br2_weights_pyro\": [],\n",
    "        \"br2_weights_bbvi\": [],\n",
    "        \"br2_weights_sdvi\": [],\n",
    "    }\n",
    "\n",
    "    for filename in file_list:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            weights = pickle.load(f)\n",
    "        for k in weight_keys:\n",
    "            weights_dict[k].append(weights[k])\n",
    "    \n",
    "    for k in weight_keys:\n",
    "        weights_dict[k] = torch.cat(weights_dict[k], 0)\n",
    "    \n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m weights_file_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# TODO: Fill out with list of files which are output of ../scripts/make_motivating_example_plot.py.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m ]\n\u001b[1;32m----> 5\u001b[0m weights_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_file_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m br2_weights_pyro \u001b[38;5;241m=\u001b[39m weights_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbr2_weights_pyro\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m br2_weights_bbvi \u001b[38;5;241m=\u001b[39m weights_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbr2_weights_bbvi\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mmerge_weights\u001b[1;34m(file_list)\u001b[0m\n\u001b[0;32m     13\u001b[0m         weights_dict[k]\u001b[38;5;241m.\u001b[39mappend(weights[k])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m weight_keys:\n\u001b[1;32m---> 16\u001b[0m     weights_dict[k] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weights_dict\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "weights_file_list = [\n",
    "    # TODO: Fill out with list of files which are output of ../scripts/make_motivating_example_plot.py.\n",
    "]\n",
    "\n",
    "weights_dict = merge_weights(weights_file_list)\n",
    "\n",
    "br2_weights_pyro = weights_dict[\"br2_weights_pyro\"]\n",
    "br2_weights_bbvi = weights_dict[\"br2_weights_bbvi\"]\n",
    "br2_weights_sdvi = weights_dict[\"br2_weights_sdvi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo2config = {\n",
    "    \"BBVI\": {\"color\": \"#56B4E9\", \"linestyle\": \"-.\"},\n",
    "    \"Pyro\": {\"color\": \"#009E73\", \"linestyle\": \":\"},\n",
    "    \"SDVI\": {\"color\": \"#D55E00\", \"linestyle\": \"-\"},\n",
    "}\n",
    "\n",
    "algo2config = {\n",
    "    \"BBVI\": {\"color\": \"#56B4E9\"},\n",
    "    \"Pyro\": {\"color\": \"#009E73\"},\n",
    "    \"SDVI\": {\"color\": \"#D55E00\"},\n",
    "}\n",
    "\n",
    "font = {'size' : 16}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "update_rc_params = {\n",
    "    'font.family': \"serif\", \n",
    "    'text.usetex': True,\n",
    "    'font.size': 20, \n",
    "    'legend.fontsize': 15,\n",
    "}\n",
    "plt.rcParams.update(update_rc_params)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#  Pyro weights\n",
    "weight_mean = br2_weights_pyro.mean(dim=0).numpy()\n",
    "weight_std = br2_weights_pyro.std(dim=0).numpy()\n",
    "ax.plot(weight_mean, lw=4, label=\"Pyro AutoGuide\", **algo2config[\"Pyro\"])\n",
    "ax.fill_between(\n",
    "    torch.arange(weight_mean.shape[0]),\n",
    "    weight_mean - weight_std,\n",
    "    weight_mean + weight_std,\n",
    "    alpha=0.5,\n",
    "    color=algo2config[\"Pyro\"][\"color\"],\n",
    ")\n",
    "\n",
    "#  BBVI weights\n",
    "weight_mean = br2_weights_bbvi.mean(dim=0).numpy()\n",
    "weight_std = br2_weights_bbvi.std(dim=0).numpy()\n",
    "ax.plot(weight_mean, lw=4, label=\"BBVI\", **algo2config[\"BBVI\"])\n",
    "ax.fill_between(\n",
    "    torch.arange(weight_mean.shape[0]),\n",
    "    weight_mean - weight_std,\n",
    "    weight_mean + weight_std,\n",
    "    alpha=0.5,\n",
    "    color=algo2config[\"BBVI\"][\"color\"],\n",
    ")\n",
    "\n",
    "#  SDVI weights\n",
    "weight_mean = br2_weights_sdvi[:,::8].mean(dim=0).numpy()\n",
    "weight_std = br2_weights_sdvi[:,::8].std(dim=0).numpy()\n",
    "ax.plot(torch.arange(weight_mean.shape[0]) * 16, weight_mean, lw=4, label=\"SDVI\", **algo2config[\"SDVI\"])\n",
    "ax.fill_between(\n",
    "    torch.arange(weight_mean.shape[0]) * 16,\n",
    "    weight_mean - weight_std,\n",
    "    weight_mean + weight_std,\n",
    "    alpha=0.5,\n",
    "    color=algo2config[\"SDVI\"][\"color\"],\n",
    ")\n",
    "\n",
    "model = ToyModel(cut_point=0.0)\n",
    "branch2_prob = (1 - model.branch1_post_prob)\n",
    "ax.axhline(branch2_prob, ls=\"--\", color=\"black\", lw=4, label=\"Ground Truth\")\n",
    "\n",
    "ax.set_xlabel(\"Number of Iterations\")\n",
    "ax.set_ylabel(r\"Probability of Branch $x \\geq 0$\")\n",
    "ax.set_yticks([0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "# ax.set_xlim(0, 1000)\n",
    "x0, y0, width, height = 0.3, 0.3, 0.4, 0.1\n",
    "ax.legend(loc=\"center\", bbox_to_anchor=(x0, y0, width, height), ncol=2)\n",
    "# ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\n",
    "    os.path.join(\"figures\", \"motivating_example_weights_sdvi.pdf\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_gtws, _ = NormalModel().calculate_ground_truth_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_extract_errors(sweep_dir, ground_truth_weights, method_name, fname=\"estimated_weights.csv\"):\n",
    "    slp_identifiers = list(ground_truth_weights.keys())\n",
    "    ground_truth_array = np.stack(\n",
    "        [np.array(ground_truth_weights[a]) for a in slp_identifiers], axis=0\n",
    "    )\n",
    "\n",
    "    experiment_dirs = [f.path for f in os.scandir(sweep_dir) if f.is_dir()]\n",
    "    dfs = []\n",
    "    for experiment_dir in experiment_dirs:\n",
    "        run_id = os.path.basename(experiment_dir)\n",
    "        if run_id == \".submitit\":\n",
    "            continue\n",
    "        \n",
    "        data = np.genfromtxt(os.path.join(experiment_dir, fname), delimiter=\",\", names=True)\n",
    "        # Make sure columns are ordered as in ground_truth_array\n",
    "        data = data[slp_identifiers]\n",
    "        # For each iteration calcluate error\n",
    "        num_iterations = data.shape[0]\n",
    "        errors = np.zeros(num_iterations)\n",
    "        for ix in range(num_iterations):\n",
    "            row_vals = np.zeros(len(slp_identifiers))\n",
    "            for j, id in enumerate(slp_identifiers):\n",
    "                row_vals[j] = data[id][ix]\n",
    "            errors[ix] = np.linalg.norm(ground_truth_array - row_vals) ** 2\n",
    "\n",
    "        # Create pandas dataframe with iteration,error as columns and append to dfs\n",
    "        errors_pd = pd.DataFrame(data={\"weight_error\": errors, \"iteration\": np.arange(num_iterations)})\n",
    "        errors_pd[\"run_id\"] = int(run_id)\n",
    "        dfs.append(errors_pd)\n",
    "    \n",
    "    # Concat all dataframes to each other\n",
    "    metrics = pd.concat(dfs)\n",
    "\n",
    "    # Group-by iteration id and calculate mean and standard deviation so that we get a table \"iteration,mean,std\"\n",
    "    grouped_by_iteration = metrics[[\"iteration\", \"weight_error\"]].groupby(\"iteration\")\n",
    "    losses_stats = grouped_by_iteration.mean()\n",
    "    losses_stats.rename(columns={\"weight_error\": \"mean\"}, inplace=True)\n",
    "    losses_stats[\"std\"] = grouped_by_iteration.std()[\"weight_error\"]\n",
    "    losses_stats[\"method_name\"] = method_name\n",
    "    return losses_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdvi_extract_errors(sweep_dir, ground_truth_weights, method_name, fname=\"exclusive_kl_results.csv\"):\n",
    "    slp_identifiers = list(ground_truth_weights.keys())\n",
    "    ground_truth_array = np.stack(\n",
    "        [np.array(ground_truth_weights[a]) for a in slp_identifiers], axis=0\n",
    "    )\n",
    "\n",
    "    # Loop through the different run idxs\n",
    "    experiment_dirs = [f.path for f in os.scandir(sweep_dir) if f.is_dir()]\n",
    "    dfs = []\n",
    "    for experiment_dir in experiment_dirs:\n",
    "        run_id = os.path.basename(experiment_dir)\n",
    "        if run_id == \".submitit\":\n",
    "            continue\n",
    "        \n",
    "        # Load metrics csv into pandas dataframe\n",
    "        metrics = pd.read_csv(os.path.join(experiment_dir, fname))\n",
    "\n",
    "        num_iterations = len(metrics.index)\n",
    "        errors = np.zeros(num_iterations)\n",
    "        for ix in range(num_iterations):\n",
    "            row_vals = np.zeros(len(slp_identifiers))\n",
    "            for j, id in enumerate(slp_identifiers):\n",
    "                # bt = int(id) * \"0\" + \"1\"\n",
    "                bt = f\"u,x_{int(id)}\"\n",
    "                row_vals[j] = metrics[f\"weight_{bt}\"][ix]\n",
    "            errors[ix] = np.linalg.norm(ground_truth_array - row_vals) ** 2\n",
    "\n",
    "        # Create pandas dataframe with iteration,error as columns and append to dfs\n",
    "        errors_pd = pd.DataFrame(data={\"weight_error\": errors, \"iteration\": np.arange(num_iterations)})\n",
    "        errors_pd[\"run_id\"] = int(run_id)\n",
    "        dfs.append(errors_pd)\n",
    "\n",
    "    # Concat all dataframes to each other\n",
    "    metrics = pd.concat(dfs)\n",
    "    metrics.describe()\n",
    "\n",
    "    # Group-by iteration id and calculate mean and standard deviation so that we get a table \"iteration,mean,std\"\n",
    "    grouped_by_iteration = metrics[[\"iteration\", \"weight_error\"]].groupby(\"iteration\")\n",
    "    losses_stats = grouped_by_iteration.mean()\n",
    "    losses_stats.rename(columns={\"weight_error\": \"mean\"}, inplace=True)\n",
    "    losses_stats[\"std\"] = grouped_by_iteration.std()[\"weight_error\"]\n",
    "    losses_stats[\"method_name\"] = method_name\n",
    "    return losses_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdvi_dirs = [\n",
    "    (\"SDVI\", \"TODO: Fill out path to results dir.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'TODO: Fill out path to results dir.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m baselines \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDCC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTODO: Fill out path to results dir.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      3\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyro AutoGuide\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTODO: Fill out path to results dir.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBBVI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTODO: Fill out path to results dir.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      5\u001b[0m ]\n\u001b[1;32m----> 7\u001b[0m method2errors \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     n: baseline_extract_errors(sweep_dir, model_selection_gtws, n)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n, sweep_dir \u001b[38;5;129;01min\u001b[39;00m baselines\n\u001b[0;32m     10\u001b[0m }\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, sweep_dir \u001b[38;5;129;01min\u001b[39;00m sdvi_dirs:\n\u001b[0;32m     12\u001b[0m     method2errors[n] \u001b[38;5;241m=\u001b[39m sdvi_extract_errors(sweep_dir, model_selection_gtws, n)\n",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m baselines \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDCC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTODO: Fill out path to results dir.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      3\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyro AutoGuide\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTODO: Fill out path to results dir.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBBVI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTODO: Fill out path to results dir.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      5\u001b[0m ]\n\u001b[0;32m      7\u001b[0m method2errors \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 8\u001b[0m     n: \u001b[43mbaseline_extract_errors\u001b[49m\u001b[43m(\u001b[49m\u001b[43msweep_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_selection_gtws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n, sweep_dir \u001b[38;5;129;01min\u001b[39;00m baselines\n\u001b[0;32m     10\u001b[0m }\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, sweep_dir \u001b[38;5;129;01min\u001b[39;00m sdvi_dirs:\n\u001b[0;32m     12\u001b[0m     method2errors[n] \u001b[38;5;241m=\u001b[39m sdvi_extract_errors(sweep_dir, model_selection_gtws, n)\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mbaseline_extract_errors\u001b[1;34m(sweep_dir, ground_truth_weights, method_name, fname)\u001b[0m\n\u001b[0;32m      2\u001b[0m slp_identifiers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ground_truth_weights\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m      3\u001b[0m ground_truth_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m      4\u001b[0m     [np\u001b[38;5;241m.\u001b[39marray(ground_truth_weights[a]) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m slp_identifiers], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m experiment_dirs \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msweep_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mis_dir()]\n\u001b[0;32m      8\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m experiment_dir \u001b[38;5;129;01min\u001b[39;00m experiment_dirs:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'TODO: Fill out path to results dir.'"
     ]
    }
   ],
   "source": [
    "baselines = [\n",
    "    (\"DCC\", \"TODO: Fill out path to results dir.\"),\n",
    "    (\"Pyro AutoGuide\", \"TODO: Fill out path to results dir.\"),\n",
    "    (\"BBVI\", \"TODO: Fill out path to results dir.\"),\n",
    "]\n",
    "\n",
    "method2errors = {\n",
    "    n: baseline_extract_errors(sweep_dir, model_selection_gtws, n)\n",
    "    for n, sweep_dir in baselines\n",
    "}\n",
    "for n, sweep_dir in sdvi_dirs:\n",
    "    method2errors[n] = sdvi_extract_errors(sweep_dir, model_selection_gtws, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(8,4))\n",
    "fig, ax = plt.subplots()\n",
    "for name, errors in method2errors.items():\n",
    "    ixs = errors.index\n",
    "    if name == \"DCC\":\n",
    "        ixs = ixs * 1000 + 10000\n",
    "        start_ix = 0\n",
    "    elif name == \"SDVI\":\n",
    "        ixs = ixs * 500 + 2000\n",
    "        start_ix = 0\n",
    "    elif name == \"Pyro AutoGuide\":\n",
    "        ixs = ixs * 1000\n",
    "        start_ix = 0\n",
    "    elif name == \"BBVI\":\n",
    "        ixs = ixs * 1000\n",
    "        start_ix = 0\n",
    "    ax.plot(\n",
    "        ixs[start_ix:], \n",
    "        errors[\"mean\"][start_ix:], \n",
    "        label=name, \n",
    "        alpha=1.0, \n",
    "        lw=4, \n",
    "        **algo2config[name]\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        ixs[start_ix:], \n",
    "        errors[\"mean\"][start_ix:]-errors[\"std\"][start_ix:], \n",
    "        errors[\"mean\"][start_ix:]+errors[\"std\"][start_ix:], \n",
    "        alpha=0.3,\n",
    "        **algo2config[name]\n",
    "    )\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Computational Cost\")\n",
    "ax.set_ylabel(\"Squared Error\")\n",
    "# ax.set_ylim((0.0, 0.05))\n",
    "# ax.set_xlim((10^4, 10^5))\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim((1000, 100000))\n",
    "# ax.set_ylim(ymin=1e-5)\n",
    "ax.grid(True)\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"figures/model_selection_slp_weight_error_without_dcc_different_aspect_ratio.pdf\")\n",
    "fig.savefig(\"figures/model_selection_slp_weight_error.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO with Marginal Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = normal_model.NormalModel()\n",
    "sdvi = SDVI(model, 0.1, \"MeanFieldNormal\", utility_class=SuccessiveHalving(10))\n",
    "sdvi.find_slps(100)\n",
    "ground_truth_weights, global_marginal_likelihood = model.calculate_ground_truth_weights(sdvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_extract_elbos(sweep_dir, method_name, fname=\"elbos.csv\"):\n",
    "    experiment_dirs = [f.path for f in os.scandir(sweep_dir) if f.is_dir()]\n",
    "    dfs = []\n",
    "    for experiment_dir in experiment_dirs:\n",
    "        run_id = os.path.basename(experiment_dir)\n",
    "        if run_id == \".submitit\":\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = np.genfromtxt(os.path.join(experiment_dir, fname), delimiter=\",\", names=True)\n",
    "        except OSError:\n",
    "            # File not found\n",
    "            continue\n",
    "\n",
    "        data = data[\"elbos\"]\n",
    "        # For each iteration calcluate error\n",
    "        num_iterations = data.shape[0]\n",
    "\n",
    "        # Create pandas dataframe with iteration,error as columns and append to dfs\n",
    "        errors_pd = pd.DataFrame(data={\"elbos\": data, \"iteration\": np.arange(num_iterations)})\n",
    "        errors_pd[\"run_id\"] = int(run_id)\n",
    "        dfs.append(errors_pd)\n",
    "    \n",
    "    # Concat all dataframes to each other\n",
    "    metrics = pd.concat(dfs)\n",
    "\n",
    "    # Group-by iteration id and calculate mean and standard deviation so that we get a table \"iteration,mean,std\"\n",
    "    grouped_by_iteration = metrics[[\"iteration\", \"elbos\"]].groupby(\"iteration\")\n",
    "    losses_stats = grouped_by_iteration.mean()\n",
    "    losses_stats.rename(columns={\"elbos\": \"mean\"}, inplace=True)\n",
    "    losses_stats[\"std\"] = grouped_by_iteration.std()[\"elbos\"]\n",
    "    losses_stats[\"method_name\"] = method_name\n",
    "    return losses_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdvi_extract_elbos(sweep_dir, method_name, fname=\"exclusive_kl_results.csv\"):\n",
    "    # Loop through the different run idxs\n",
    "    experiment_dirs = [f.path for f in os.scandir(sweep_dir) if f.is_dir()]\n",
    "    dfs = []\n",
    "    for experiment_dir in experiment_dirs:\n",
    "        run_id = os.path.basename(experiment_dir)\n",
    "        if run_id == \".submitit\":\n",
    "            continue\n",
    "\n",
    "        # Load metrics csv into pandas dataframe\n",
    "        metrics = pd.read_csv(os.path.join(experiment_dir, fname))\n",
    "\n",
    "        num_iterations = len(metrics.index)\n",
    "\n",
    "        # Create pandas dataframe with iteration,error as columns and append to dfs\n",
    "        errors_pd = pd.DataFrame(data={\"elbos\": metrics[\"global_elbos\"], \"iteration\": np.arange(num_iterations)})\n",
    "        errors_pd[\"run_id\"] = int(run_id)\n",
    "        dfs.append(errors_pd)\n",
    "\n",
    "    # Concat all dataframes to each other\n",
    "    metrics = pd.concat(dfs)\n",
    "    metrics.describe()\n",
    "\n",
    "    # Group-by iteration id and calculate mean and standard deviation so that we get a table \"iteration,mean,std\"\n",
    "    grouped_by_iteration = metrics[[\"iteration\", \"elbos\"]].groupby(\"iteration\")\n",
    "    losses_stats = grouped_by_iteration.mean()\n",
    "    losses_stats.rename(columns={\"elbos\": \"mean\"}, inplace=True)\n",
    "    losses_stats[\"std\"] = grouped_by_iteration.std()[\"elbos\"]\n",
    "    losses_stats[\"method_name\"] = method_name\n",
    "    return losses_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'TODO: Fill out path to results dir.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m method2elbos \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     name: baseline_extract_elbos(d, name) \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, d \u001b[38;5;129;01min\u001b[39;00m baselines \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyro AutoGuide\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBBVI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m }\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, sweep_dir \u001b[38;5;129;01min\u001b[39;00m sdvi_dirs:\n\u001b[0;32m      7\u001b[0m     method2elbos[n] \u001b[38;5;241m=\u001b[39m sdvi_extract_elbos(sweep_dir, n)\n",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m method2elbos \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 2\u001b[0m     name: \u001b[43mbaseline_extract_elbos\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, d \u001b[38;5;129;01min\u001b[39;00m baselines \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyro AutoGuide\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBBVI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m }\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, sweep_dir \u001b[38;5;129;01min\u001b[39;00m sdvi_dirs:\n\u001b[0;32m      7\u001b[0m     method2elbos[n] \u001b[38;5;241m=\u001b[39m sdvi_extract_elbos(sweep_dir, n)\n",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m, in \u001b[0;36mbaseline_extract_elbos\u001b[1;34m(sweep_dir, method_name, fname)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbaseline_extract_elbos\u001b[39m(sweep_dir, method_name, fname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melbos.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     experiment_dirs \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msweep_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mis_dir()]\n\u001b[0;32m      3\u001b[0m     dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m experiment_dir \u001b[38;5;129;01min\u001b[39;00m experiment_dirs:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'TODO: Fill out path to results dir.'"
     ]
    }
   ],
   "source": [
    "method2elbos = {\n",
    "    name: baseline_extract_elbos(d, name) \n",
    "    for name, d in baselines \n",
    "    if name in [\"Pyro AutoGuide\", \"BBVI\"]\n",
    "}\n",
    "for n, sweep_dir in sdvi_dirs:\n",
    "    method2elbos[n] = sdvi_extract_elbos(sweep_dir, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_width = 4\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "for name, errors in method2elbos.items():\n",
    "    ixs = errors.index\n",
    "    if name == \"SDVI\":\n",
    "        ixs = ixs * 500 + 2000\n",
    "    elif name == \"Pyro AutoGuide\":\n",
    "        ixs = ixs * 1000\n",
    "    elif name == \"BBVI\":\n",
    "        ixs = ixs * 1000\n",
    "    ax.plot(ixs, errors[\"mean\"], alpha=1.0, lw=line_width, **algo2config[name])\n",
    "    ax.fill_between(ixs, errors[\"mean\"]-errors[\"std\"], errors[\"mean\"]+errors[\"std\"], alpha=0.3, **algo2config[name])\n",
    "\n",
    "ax.axhline(\n",
    "    torch.log(global_marginal_likelihood),\n",
    "    linestyle=\"--\",\n",
    "    color=\"black\",\n",
    "    lw=line_width,\n",
    "    label=r\"$\\log Z$\"\n",
    ")\n",
    "ax.set_xlabel(\"Computational Cost\")\n",
    "ax.set_ylabel(\"ELBO\")\n",
    "# ax.set_ylim((0.0, 0.05))\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim((1000, 100000))\n",
    "\n",
    "ax.grid(True)\n",
    "ax.legend(loc=\"lower right\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/model_selection_elbos.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP Kernel Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Posterior Predictive Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"../data/airline/airline.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_with_median_lppd(sweep_dir, key=\"lppds\", fname=\"exclusive_kl_results.csv\"):\n",
    "    # Loop through the different run idxs\n",
    "    experiment_dirs = [f.path for f in os.scandir(sweep_dir) if f.is_dir()]\n",
    "    print(experiment_dirs)\n",
    "    dfs = []\n",
    "    for experiment_dir in experiment_dirs:\n",
    "        run_id = os.path.basename(experiment_dir)\n",
    "        if run_id == \".submitit\":\n",
    "            continue\n",
    "        # Load metrics csv into pandas dataframe\n",
    "        metrics = pd.read_csv(os.path.join(experiment_dir, fname))\n",
    "\n",
    "        # Create pandas dataframe with iteration,error as columns and append to dfs\n",
    "        errors_pd = pd.DataFrame(data={key: metrics[key].iloc[-1], \"iteration\": [0]})\n",
    "        errors_pd[\"run_id\"] = int(run_id)\n",
    "        dfs.append(errors_pd)\n",
    "\n",
    "    # Concat all dataframes to each other\n",
    "    metrics = pd.concat(dfs)\n",
    "    print(metrics)\n",
    "    median_iteration_id = metrics.loc[metrics[\"lppds\"] == metrics[\"lppds\"].median()][\"run_id\"][0]\n",
    "    return median_iteration_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../experiments/gp_grammar_sdvi/17-25-14\\\\0']\n",
      "       lppds  iteration  run_id\n",
      "0 -31.334535          0       0\n"
     ]
    }
   ],
   "source": [
    "# sweep_dir = gp_sdvi_result_dirs[1][1]\n",
    "sweep_dir = \"../experiments/gp_grammar_sdvi/17-25-14\"\n",
    "median_lppd_id = get_id_with_median_lppd(sweep_dir)\n",
    "# median_lppd_id = 1\n",
    "with open(os.path.join(sweep_dir, str(median_lppd_id), \"sdvi.pickle\"), \"rb\") as f:\n",
    "    sdvi = pickle.load(f).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    data = torch.tensor(np.loadtxt(data_path, delimiter=\",\"))\n",
    "    xs = data[:, 0]\n",
    "    ys = data[:, 1]\n",
    "    xs -= xs.min()\n",
    "    xs /= xs.max()\n",
    "    ys -= ys.mean()\n",
    "    ys *= 4 / (ys.max() - ys.min())\n",
    "\n",
    "    # Keep 10 % of data for validation.\n",
    "    val_ix = round(xs.size(0) * 0.9)\n",
    "    xs, xs_val = xs[:val_ix], xs[val_ix:]\n",
    "    ys, ys_val = ys[:val_ix], ys[val_ix:]\n",
    "\n",
    "    return xs, ys, xs_val, ys_val\n",
    "\n",
    "def extract_posterior_kernels(posterior_samples):\n",
    "    post_kernels = [trace.nodes[\"_RETURN\"][\"value\"] for trace in posterior_samples]\n",
    "    for ix in range(len(post_kernels)):\n",
    "        for name, s in posterior_samples[ix].iter_stochastic_nodes():\n",
    "            if name in [\"std\", \"y\"] or \"kernel_type\" in name:\n",
    "                continue\n",
    "\n",
    "            if isinstance(post_kernels[ix], gp.kernels.Sum) or isinstance(\n",
    "                post_kernels[ix], gp.kernels.Product\n",
    "            ):\n",
    "                names = name.split(\".\")\n",
    "                kern_mod = post_kernels[ix]._modules[names[0]]\n",
    "                for jx in range(len(names) - 2):\n",
    "                    kern_mod = kern_mod._modules[names[jx + 1]]\n",
    "                setattr(kern_mod, names[-1], s[\"value\"])\n",
    "            else:\n",
    "                setattr(post_kernels[ix], name, s[\"value\"])\n",
    "    return post_kernels\n",
    "\n",
    "def gp_analytic_posterior(\n",
    "    kernel_fn: gp.kernels.Kernel,\n",
    "    X: torch.tensor,\n",
    "    new_xs: torch.tensor,\n",
    "    y: torch.tensor,\n",
    "    noise: torch.tensor,\n",
    "    jitter: float,\n",
    "    full_cov: bool = False,\n",
    "):\n",
    "    N = X.size(0).to(\"cuda\")\n",
    "    Kff = kernel_fn(X).contiguous().to(\"cuda\")\n",
    "    Kff = Kff.type(X.dtype).clone().to(\"cuda\")\n",
    "    Kff.view(-1)[:: N + 1] += jitter + torch.pow(noise, 2)\n",
    "    Lff = torch.linalg.cholesky(Kff).to(\"cuda\")\n",
    "\n",
    "    gp_post_mean, gp_post_cov = gp.util.conditional(\n",
    "        new_xs, X, kernel_fn, y, Lff=Lff, jitter=jitter, full_cov=full_cov\n",
    "    )\n",
    "    if full_cov:\n",
    "        M = new_xs.size(0).to(\"cuda\")\n",
    "        gp_post_cov = gp_post_cov.contiguous().to(\"cuda\")\n",
    "        gp_post_cov.view(-1, M * M)[:, :: M + 1] += torch.pow(noise, 2)\n",
    "    else:\n",
    "        gp_post_cov = gp_post_cov + torch.pow(noise, 2).to(\"cuda\")\n",
    "    return gp_post_mean, gp_post_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior_samples(\n",
    "    posterior_samples, \n",
    "    X, \n",
    "    y, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    jitter=1e-6, \n",
    "    with_noise=True, \n",
    "    num_eval_points=500,\n",
    "    start_ix_data=0,\n",
    "    figsize=(15, 10)\n",
    "):\n",
    "    post_kernels = extract_posterior_kernels(posterior_samples)\n",
    "    if with_noise:\n",
    "        noises = [trace.nodes[\"std\"][\"value\"] for trace in posterior_samples]\n",
    "    else:\n",
    "        noises = [torch.tensor(0.0) for _ in range(len(posterior_samples))]\n",
    "\n",
    "    new_xs = torch.linspace(0, 1, num_eval_points)\n",
    "    posterior_fs = torch.zeros((len(post_kernels), new_xs.size(0)))\n",
    "    for ix in range(len(post_kernels)):\n",
    "        with torch.no_grad():\n",
    "            gp_post_mean, gp_post_cov = gp_analytic_posterior(\n",
    "                post_kernels[ix],\n",
    "                X,\n",
    "                new_xs,\n",
    "                y,\n",
    "                noises[ix],\n",
    "                jitter,\n",
    "                full_cov=True,\n",
    "            )\n",
    "        posterior_fs[ix, :] = (\n",
    "            dist.MultivariateNormal(gp_post_mean, gp_post_cov).sample().detach()\n",
    "        )\n",
    "\n",
    "    f_post_mean = posterior_fs.mean(dim=0)\n",
    "    f_post_std = posterior_fs.std(dim=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot(new_xs, f_post_mean, color=\"#0072B2\", lw=2)\n",
    "    ax.fill_between(\n",
    "        new_xs,\n",
    "        f_post_mean - 2 * f_post_std,\n",
    "        f_post_mean + 2 * f_post_std,\n",
    "        color=\"#0072B2\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    num_samples_to_plot = min(0, len(post_kernels))\n",
    "    for ix in range(num_samples_to_plot):\n",
    "        ax.plot(new_xs, posterior_fs[ix, :], color=\"#009E73\", alpha=0.3, linestyle=\"-\")\n",
    "\n",
    "    ax.scatter(X, y, label=\"Observed Data\", color=\"black\")\n",
    "    ax.scatter(X_val, y_val, label=\"Held-Out Data\", marker=\"x\")\n",
    "    ax.set_xlim((X[start_ix_data] - 0.01, 1.01))\n",
    "    ax.set_ylim((y[start_ix_data:].min() - 0.1, ax.get_ylim()[1]))\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    \n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Number of Passengers\")\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_val, y_val = load_data(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate sdvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = sdvi.sample_posterior_predictive(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_posterior_samples(\n",
    "    posterior_samples, \n",
    "    X, \n",
    "    y, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    num_eval_points=1000, \n",
    "    start_ix_data=70, \n",
    "    figsize=(8, 5)\n",
    ")\n",
    "fig.savefig(\"figures/gp_posterior_predictive_median_lppd.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc0cf9eb75f14dfd7d934912c4868fd66671e39dd54480a150159752595feba5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
